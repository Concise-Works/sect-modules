

\section{Asymptotic Notation}

Asymptotic analysis is a method for describing the limiting behavior of functions as inputs grow infinitely.

\begin{Def}[Asymptotic]
    
    Let \(f(n)\) and \(g(n)\) be two functions. As \(n\) grows, if \(f(n)\) grows closer to \(g(n)\) never reaching, we say that \underline{``$f(n)$ is \textbf{asymptotic} to \(g(n)\).''}\\
    
    \noindent
    We call the point where \(f(n)\) starts behaving similarly to \(g(n)\) the \underline{\textbf{threshold} \(n_0\).} After this point $n_0$, \(f(n)\) follows the same general path as \(g(n)\).
\end{Def}

\begin{Def}[Big-O: (Upper Bound)]

    Let $f$ and $g$ be functions. $f(n)$ our function of interest, and $g(n)$
    our function of comparison.\\

    \noindent
    Then we say $f(n)=O(g(n))$, ``$f(n)$ \textbf{is big-O of} $g(n)$,'' if $f(n)$ 
    grows no faster than $g(n)$, up to a constant factor.
    Let $n_0$ be our asymptotic threshold. Then, for all $n\geq n_0$,
    \large
    \[0\leq f(n) \leq c\cdot g(n)\]
    \normalsize
    
    \noindent
    Represented as the ratio $\dfrac{f(n)}{g(n)}\leq c$ for all $n\geq n_0$. Analytically we write,
    \Large
    \[\lim_{n\to\infty}\dfrac{f(n)}{g(n)}< \infty\]
    \normalsize
    \noindent
    Meaning, as we chase infinity, our numerator grows slower than the denominator, bounded, never reaching infinity. 
\end{Def}

\newpage

\noindent
\textbf{Examples:}
\begin{enumerate}
    \item[(i.)] $3n^2+2n+1=O(n^2)$
    \item[(ii.)] $n^{100}=O(2^n)$
    \item[(iii.)] $\log n=O(\sqrt{n})$ 
\end{enumerate}

\begin{Proof}[$\log n = O(\sqrt{n})$]
We setup our ratio:
\[\lim_{n\to\infty}\dfrac{\log n}{\sqrt{n}}\]
\noindent
Since $\log n$ and $\sqrt{n}$ grow infinitely without bound, they are of indeterminate form $\frac{\infty}{\infty}$. We apply L'Hopital's Rule, which states
that taking derivatives of the numerator and denominator will yield an evaluateable limit:
\Large
\[\lim_{n\to\infty}\dfrac{\log n}{\sqrt{n}}=\lim_{n\to\infty}\dfrac{\frac{d}{dn}\log n}{\frac{d}{dn}\sqrt{n}}\]
\normalsize
\noindent
Yielding derivatives, $\log n = \frac{1}{n}$ and $\sqrt{n}=\frac{1}{2\sqrt{n}}$. We substitute these back into our limit:
\Large
\[\lim_{n\to\infty}\dfrac{\frac{1}{n}}{\frac{1}{2\sqrt{n}}}=\lim_{n\to\infty}\dfrac{2\sqrt{n}}{n}=\lim_{n\to\infty}\dfrac{2}{\sqrt{n}}=0\]
\normalsize
\noindent
Our limit approaches 0, as we have a constant factor in the numerator, and a growing denominator. Thus, $\log n = O(\sqrt{n})$, as $0<\infty$.
\end{Proof}

\begin{Def}[Big-$\Omega$: (Lower Bound)]

    The symbol $\Omega$ reads ``Omega.'' Let $f$ and $g$ be functions. Then 
    $f(n)=\Omega(g(n))$ if $f(n)$ grows no slower than $g(n)$, up to a constant factor. I.e.,
    lower bounded by $g(n)$. Let $n_0$ be our asymptotic threshold. Then, for all $n\geq n_0$,
    \large
    \[0\leq c\cdot g(n) \leq f(n)\]
    \[0<\lim_{n\to\infty}\dfrac{f(n)}{g(n)}\]
    \normalsize
    \noindent
    Meaning, as we chase infinity, our numerator grows faster than the denominator, approaching 0 asymptotically.
\end{Def}

\noindent
\textbf{Examples:} $n!=\Omega(2^n)$; $\dfrac{n}{100}= \Omega(n)$; $n^{3/2}= \Omega(\sqrt{n})$; $\sqrt{n} = \Omega(\log n)$

\newpage

\begin{Def}[Big $\Theta$: (Tight Bound)]

    The symbol $\Theta$ reads ``Theta.'' Let $f$ and $g$ be functions. Then $f(n)=\Theta(g(n))$ if $f(n)$ grows at the same rate as $g(n)$, up to a constant factor. I.e., $f(n)$ is both upper and lower bounded by $g(n)$. Let $n_0$ be our asymptotic threshold, and $c_1>0,c_2>0$ be some constants. Then, for all $n\geq n_0$,
    \large
    \[0\leq c_1\cdot g(n) \leq f(n) \leq c_2\cdot g(n)\]
    \[0<\lim_{n\to\infty}\dfrac{f(n)}{g(n)}<\infty\]
    \normalsize
    \noindent
    Meaning, as we chase infinity, our numerator grows at the same rate as the denominator.
\end{Def}
\noindent
\textbf{Examples:} $n^2=\Theta(n^2)$; $2n^3+2n=\Theta(n^3)$; $\log n+\sqrt{n}=\Theta(\sqrt{n})$.
\begin{Def}[Little $o$: (Strict Upper Bound)]

    The symbol $o$ reads ``little-o.'' Let $f$ and $g$ be functions. Then $f(n)=o(g(n))$ if $f(n)$ grows strictly slower than $g(n)$, meaning $f(n)$ becomes insignificant compared to $g(n)$ as $n$ grows large. Let $n_0$ be our asymptotic threshold. Then, for all $n\geq n_0$,
    \large
    \[0\leq f(n) < c \cdot g(n)\]
    \[\lim_{n\to\infty}\dfrac{f(n)}{g(n)}=0\]
    \normalsize
    \noindent
    Meaning, as we chase infinity, the ratio of $f(n)$ to $g(n)$ shrinks to zero.
\end{Def}
\noindent
\textbf{Examples:} $n=\!o(n^2)$; $\log n=\!o(n)$; $n^{0.5}=\!o(n)$.
\begin{Def}[Little $\omega$: (Strict Lower Bound)]

    The symbol $\omega$ reads ``little-omega.'' Let $f$ and $g$ be functions. Then $f(n)=\omega(g(n))$ if $f(n)$ grows strictly faster than $g(n)$, meaning $g(n)$ becomes insignificant compared to $f(n)$ as $n$ grows large. Let $n_0$ be our asymptotic threshold. Then, for all $n\geq n_0$,
    \large
    \[0\leq c \cdot g(n) < f(n)\]
    \[\lim_{n\to\infty}\dfrac{g(n)}{f(n)}=0\]
    \normalsize
    \noindent
    Meaning, as we chase infinity, the ratio of $g(n)$ to $f(n)$ shrinks to zero.
\end{Def}
\noindent
\textbf{Examples:} $n^2=\omega(n)$; $n=\omega(\log n)$.

\newpage 

\begin{Def}[Asymptotic Equality (\( \sim \))]

    The symbol \( \sim \) reads ``asymptotic equality.'' Let $f$ and $g$ be functions. Then $f(n) \sim g(n)$ if, as $n \to \infty$, the ratio of $f(n)$ to $g(n)$ approaches 1. I.e., the two functions grow at the same rate asymptotically. Formally,
    \large
    \[\lim_{n\to\infty}\dfrac{f(n)}{g(n)}=1\]
    \normalsize
    \noindent
    Meaning, as $n$ grows large, the two functions become approximately equal.
\end{Def}
\noindent
\textbf{Examples:} $n + 100 \sim n, \quad \log(n^2) \sim 2\log n.$


\begin{Tip}
    To review:
    \begin{itemize}
        \item \textbf{Big-$O$:} $f(n)$ < $g(n)$ (Upper Bound); $f(n)$ grows no faster than $g(n)$.
        \item \textbf{Big-$\Omega$:} $f(n)$ > $g(n)$ (Lower Bound); $f(n)$ grows no slower than $g(n)$.
        \item \textbf{Big-$\Theta$:} $f(n)$ = $g(n)$ (Tight Bound); $f(n)$ grows at the same rate as $g(n)$.
        \item \textbf{Little-$o$:} $f(n)$ < $g(n)$ (Strict Upper Bound); $f(n)$ grows strictly slower than $g(n)$.
        \item \textbf{Little-$\omega$:} $f(n)$ > $g(n)$ (Strict Lower Bound); $f(n)$ grows strictly faster than $g(n)$.
        \item \textbf{Asymptotic Equality:} $f(n)$ $\sim$ $g(n)$; $f(n)$ grows at the same rate as $g(n)$.
    \end{itemize}
\end{Tip}

\begin{theo}[Types of Asymptotic Behavior]

    The following are common relationships between different types of functions and their asymptotic growth rates:

    \begin{itemize}
        \item \textbf{Polynomials.} Let $f(n) = a_0 + a_1 n + \dots + a_d n^d$ with $a_d > 0$. Then, \underline{$f(n)$ is $\Theta(n^d)$.}\\
        E.e., $3n^2+2n+1$ is $\Theta(n^2)$.
        
        \item \textbf{Logarithms.} \underline{$\Theta(\log_a n)$ is $\Theta(\log_b n)$} for any constants $a, b > 0$. That is, logarithmic functions in different bases have the same growth rate.\\
        E.g., $\log_2 n$ is $\Theta(\log_3 n)$.
        
        \item \textbf{Logarithms and Polynomials.} For every $d > 0$, \underline{$\log n$ is $O(n^d)$.} This indicates that logarithms grow slower than any polynomial.\\
        E.g., $\log n$ is $O(n^2)$.
        
        \item \textbf{Exponentials and Polynomials.} For every $r > 1$ and every $d > 0$, \underline{$n^d$ is $O(r^n)$.} This means that exponentials grow faster than any polynomial.\\
        E.e., $n^2$ is $O(2^n)$.
    \end{itemize}
\end{theo}

\newpage 
\section{Evaluating Algorithms}
\noindent
When analyzing algorithms, we are interested in two primary factors: time and space complexity.

\begin{Def}[Time Complexity]
    
        The \textbf{time complexity} of an algorithm is the amount of time it takes to run as a function of the input size. We use asymptotic notation to describe the time complexity of an algorithm.
\end{Def}

\begin{Def}[Space Complexity]
    
        The \textbf{space complexity} of an algorithm is the amount of memory it uses to store inputs and subsequent variables during the algorithm's execution. We use asymptotic notation to describe the space complexity of an algorithm.
\end{Def}
\noindent
Below is an example of a function and its time and space complexity analysis.

\begin{Func}[Arithmetic Series - \texttt{Fun1($A$)}]
    Computes a result based on a length-$n$ array of integers:

    \vspace{.5em}
    \noindent
    \textbf{Input: } A length-$n$ array of integers.\\
    \textbf{Output: } An integer $p$ computed from the array elements.\\
    \begin{algorithm}[H]
        \SetAlgoLined
        
        \vspace{.5em}
        \SetKwProg{Fn}{Function}{:}{\KwRet{$p$}}
        \Fn{\texttt{Fun1($A$)}}{
            $p \gets 0$\;
            \For{$i \gets 1$ \KwTo $n-1$}{
                \For{$j \gets i+1$ \KwTo $n$}{
                    $p \gets p + A[i] \cdot A[j]$\;
                }
            }
        }
    \end{algorithm}

    \noindent
    \textbf{Time Complexity:} For $f(n):=$ \texttt{Fun1($A$)}, $f(n)=\frac{n^2}{2}=O(n^2)$. This is because the function has a nested loop structure, where the inner for-loop runs $n-i$ times, and the outer for-loop runs $n-1$ times. Thus, the total number of iterations is $\sum_{i=1}^{n-1}n-i=\frac{n^2}{2}$.\\

    \noindent
    \textbf{Space Complexity:} We yield $O(n)$ for storing an array of length $n$. The variable $p$ is $O(1)$ (constant), as it is a single integer. Hence, $f(n)=n+1=O(n)$.
    \end{Func}   

    \noindent
    \textbf{Additional Example:} Let $f(n,m) = n^2m + m^3 + nm^3$. Then, $f(n,m)=O(n^2m+m^3)$. This is because both $n$ and $m$ must be accounted for. Our largest $n$ term is $n^2m$, and our largest $m$ term is $m^3$ both dominate the expression. Thus, $f(n,m)=O(n^2m+m^3)$.



